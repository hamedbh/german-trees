---
title: "ML Course Model Training"
output: 
  html_notebook: 
    theme: journal
    toc: yes
    toc_depth: 2
---

This is an expanded version of the modelling work done on the course in Bootle. Data are downloaded directly from the [UCI Machine Learning Repository][1]. Models built using [XGBoost][2] and random forests.

# Data Loading and Setup

```{r load libraries}
# load libraries for the work, comments indicate what each is for
suppressPackageStartupMessages({
    library(dplyr) # data wrangling
    library(data.table) # for handling evaluation logs from xgboost
    library(tidyr) # reshaping data sets
    library(purrr) # functional programming package
    library(corrplot) # easy plotting of correlation matrix
    library(caret) # machine learning
    library(xgboost) # machine learning
    library(Matrix) # for creating model matrices
    library(readr) # for reading data direct to tibbles
    library(ggplot2) # data viz
    library(viridis) # colour palettes for plots
    library(stringi) # string manipulation
    library(pROC) # handling ROC curves
    library(forcats) # managing factors/categorical variables
    library(knitr) # pretty printing of tables with kable()
    library(rBayesianOptimization)
    source("./R/print_conf_matrix.R")
})
```

Now get the data files from UCI, unless they are already present in the `./data/` directory.

```{r reading data}
# url for the main dataset
data_url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data"
# the data dictionary
data_dict_url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc"

# local filepaths
data_path <- paste0("./data/", basename(data_url))
dict_path <- paste0("./data/", basename(data_dict_url))

# download files if not already present
if (!file.exists(data_path)) {
    download.file(data_url, data_path)
}
if (!file.exists(dict_path)) {
    download.file(data_dict_url, dict_path)
}

# column names taken from the data dictionary, slightly changed to keep lengths 
# reasonable
column_names <- c("acct_status", "duration", "credit_history", 
                  "purpose", "amount", "savings_acct", 
                  "present_emp_since", "pct_of_income", "sex_status", 
                  "other_debtor_guarantor", "resident_since", "property", 
                  "age", "other_debts", "housing", 
                  "num_existing_credits", "job", "num_dependents", 
                  "telephone", "foreign_worker", "outcome")
# set the column types manually to avoid any coercion errors
column_types <- c("ciccicciccicicciciccc")

raw_df <- read_delim(data_path, 
                     delim = " ", 
                     col_names = column_names, 
                     col_types = column_types)
```

# Exploratory Data Analysis

`readr::read_delim()` does not convert strings to factors automatically. This means I must set factor levels manually, but this allows for greater control over the levels and means I can give them more informative names from the start. An alternative would be to use the base `utils::read.table()` function, which defaults to `stringsAsFactors = TRUE`, and then rename factors later.

```{r}
# details of the factors are taken from the data dictionary
# Can ignore warnings, which are because there are no rows with the given 
# Axx code. These manipulations use functions from the forcats package, 
# which makes factors much easier
clean_df <- raw_df %>% 
    mutate(acct_status = fct_recode(acct_status, 
                                    overdrawn = "A11", below_200DM = "A12", 
                                    over_200DM = "A13", no_acct = "A14"), 
           credit_history = fct_recode(credit_history, 
                                       none_taken_all_paid = "A30", 
                                       all_paid_this_bank = "A31", 
                                       all_paid_duly = "A32", 
                                       past_delays = "A33", 
                                       critical_acct = "A34"), 
           purpose = fct_recode(purpose, 
                                car_new = "A40", car_used = "A41", 
                                furniture_equipment  = "A42", 
                                radio_tv = "A43",  
                                dom_appliance = "A44", 
                                repairs = "A45", 
                                education = "A46", 
                                vacation = "A47", 
                                retraining = "A48", 
                                business = "A49",  
                                others = "A410"), 
           savings_acct = fct_recode(savings_acct, 
                                     to_100DM = "A61", 
                                     to_500DM = "A62", 
                                     to_1000DM = "A63", 
                                     over_1000DM = "A64", 
                                     unknwn_no_acct = "A65"),
           present_emp_since = fct_recode(present_emp_since, 
                                          unemployed = "A71", 
                                          to_1_yr = "A72", 
                                          to_4_yrs = "A73", 
                                          to_7_yrs = "A74", 
                                          over_7_yrs = "A75"), 
           sex_status = fct_recode(sex_status, 
                                   male_divorced = "A91", 
                                   female_married = "A92", 
                                   male_single = "A93",  
                                   male_married = "A94", 
                                   female_single = "A95"), 
           other_debtor_guarantor = fct_recode(other_debtor_guarantor, 
                                               none = "A101",  
                                               co_applicant = "A102", 
                                               guarantor = "A103"), 
           property = fct_recode(property, 
                                 real_estate = "A121", 
                                 savings_insurance = "A122", 
                                 car_other = "A123", 
                                 unknwn_none = "A124"), 
           other_debts = fct_recode(other_debts, 
                                    bank = "A141", 
                                    stores = "A142", 
                                    none = "A143"), 
           housing = fct_recode(housing, 
                                rent = "A151", 
                                own = "A152", 
                                for_free = "A153"), 
           job = fct_recode(job, 
                            unemp_unskilled_nonres = "A171", 
                            unskilled_res = "A172",  
                            skilled_official = "A173", 
                            mgmt_highqual = "A174"), 
           telephone = fct_recode(telephone, 
                                  no = "A191", 
                                  yes = "A192"), 
           foreign_worker = fct_recode(foreign_worker, 
                                       yes = "A201", 
                                       no = "A202"), 
           outcome = fct_recode(outcome, 
                                good = "1", 
                                bad = "2")) %>% 
    # add another factor for gender, a simplification of sex_status, which can 
    # then be compared during EDA
    mutate(gender = fct_collapse(sex_status, 
                                 male = "male_divorced", 
                                 male = "male_single", 
                                 male = "male_married", 
                                 female = "female_married", 
                                 female = "female_single"))

glimpse(clean_df)
```

I want to check for homogeneity and/or small classes among the categorical variables.

```{r}
factor_varnames <- colnames(clean_df)[sapply(clean_df, is.factor)]
cat_pct <- 0.02
walk(factor_varnames, function(x) {
    tmp_tbl <- fct_count(clean_df[[x]]) %>% 
        mutate(pct = n/sum(n))
    if (min(tmp_tbl[["pct"]]) < cat_pct) {
        print(paste0(x, " has categories with less than ", 
                     as.integer(100 * cat_pct), "% of observations."))
        print(table(clean_df[[x]])/nrow(clean_df))
    }
})

```
Now I can start visualising the distributions of the variables to see if any other patterns emerge.

```{r}
# start with bar plots for the factor vars
clean_df %>% 
    dplyr::select(-outcome) %>% 
    select_if(is.factor) %>% 
    mutate_all(as.character) %>% # this is to avoid an error about factor 
    # attributes being lost, but this step isn't 
    # strictly necessary as gather() would coerce 
    # to character anyway
    gather(key = "feature") %>% # gather() turns a wide table to a long one
    ggplot(aes(x = value)) + 
    geom_bar() + 
    facet_wrap(~ feature, scales = "free_x") + 
    theme_minimal() + 
    theme(axis.text.x = element_blank())

# then histograms for integers
clean_df %>% 
    dplyr::select_if(is.integer) %>% 
    gather(key = "feature") %>% 
    ggplot(aes(x = value)) +
    geom_histogram(bins = 20) +
    facet_wrap(~ feature, scales = "free_x") + 
    theme_minimal() +
    theme(axis.text.x = element_blank())
```

`foreign_worker`, `other_debtor_guarantor`, and `sex_status` seem to have some very small categories, and are worth more examination.

```{r}
walk(c("foreign_worker", "other_debtor_guarantor", "sex_status"), 
     function(x) {
         tmp_tbl <- fct_count(clean_df[[x]]) %>% 
             mutate(pct = n/sum(n))
         # print(paste0(x, " has categories with less than ", 
         #              as.integer(100 * cat_pct), "% of observations."))
         print(paste0("Proportions for variable: ", x))
         print(table(clean_df[[x]])/nrow(clean_df))
     })
```

Given that I'll be using decision tree methods these imbalances should not affect the results too much, but for logistic regression it would be more of a problem.

I also want to compare the `sex_status` and `gender` variables, to see whether keeping just gender would make sense.

```{r}
clean_df %>% 
    select(sex_status, gender) %>% 
    mutate_all(as.character) %>% 
    gather(key = "feature") %>% 
    ggplot(aes(x = value)) + 
    geom_bar() + 
    facet_wrap(~ feature, scales = "free_x") + 
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

Keeping both variables doesn't make intuitive sense, and neither does having a model so imbalanced towards men. Although a tree model would probably distinguish between them I will drop `sex_status` before proceeding.


```{r}
clean_df$sex_status <- NULL
```

This is the point at which I'll partition the data, before examining correlations between continuous variables.

```{r}
# set a seed to make analysis reproducible
set.seed(1705L)
# caret::createDataPartition() will ensure that the overall proportions are 
# reflected in each partition
in_train <- createDataPartition(clean_df[["outcome"]], 
                                p = 0.8, 
                                list = FALSE)
train_df <- clean_df[in_train, ]
test_df <- clean_df[-in_train, ]
```

Now examine correlations between continuous variables.

```{r}
corr_matrix <- train_df %>% 
    select_if(is.integer) %>% 
    data.matrix() %>% 
    cor()
corrplot(corr_matrix)
```

The correlation between `amount` and `duration` seems high, and worth examining further.

```{r}
cor(train_df$amount, train_df$duration)
```

A correlation value of `r round(cor(train_df$amount, train_df$duration), 2)` is a little high. Which has the closest relationship with `outcome`?

```{r}
# set up simple logistic regression for each of the variables with outcome
amount_model <- glm(outcome ~ amount, 
                    family = binomial(link = "logit"), 
                    data = train_df)
duration_model <- glm(outcome ~ duration, 
                      family = binomial(link = "logit"), 
                      data = train_df)
# examine the summary of residuals
summary(amount_model$residuals)
summary(duration_model$residuals)
```

There is not much difference between them: if I were using a method such as logistic regression for the full model I would need to pick one, but I can afford to put both variables into a tree-based model, starting with a random forest.

# Random Forest with AUC Objective

I will train the random forest through `caret::train()`, which allows for easy grid search on the `mtry` parameter in cross-validation.

NB. Both here and when validating/training the XGBoost model I write the model objects to disk as `.rds` files, and read those in if possible. This is simply to save time retraining models when running the notebook again. If you want to force the model to train again simply delete the relevant `.rds` file from the `data` directory.

```{r}
# set up trainControl object to pass to caret::train()
rf_trcontrol <- trainControl(method = "repeatedcv", 
                             # 5-fold cross-validationm repeated 3 times
                             number = 5L, 
                             repeats = 3L,
                             summaryFunction = twoClassSummary, 
                             search = "random", 
                             classProbs = TRUE, 
                             savePredictions = "final")
# set up tuning grid for testing values of mtry
rf_tunegrid <- expand.grid(.mtry = seq(2, 20, by = 2))

if (file.exists("./data/rf_model.rds")) {
    rf_model <- read_rds("./data/rf_model.rds")
} else {
    rf_model <- train(outcome ~ ., 
                      data = train_df, 
                      method = "rf", 
                      metric = "ROC", 
                      maximize = TRUE, 
                      tuneGrid = rf_tunegrid,
                      nodesize = 20L, 
                      ntree = 1500L, 
                      trControl = rf_trcontrol)
    
    write_rds(rf_model, "./data/rf_model.rds")
}
print(rf_model)
plot(rf_model)
```

It seems that `r rf_model[["results"]][["mtry"]][which.max(rf_model[["results"]][["ROC"]])]` is the value for `mtry` that maximises AUC. I can now use `caret::thresholder()` to get the optimum threshold for the predicted probabilities.

```{r}
rf_thresholds <- thresholder(rf_model, seq(0.01, 0.99, 0.01))
rf_best_threshold <- rf_thresholds[["prob_threshold"]][[which.min(rf_thresholds[["Dist"]])]]
paste0("Threshold that maximises AUC is ", rf_best_threshold)
```

Now evaluate the model on the test set.

```{r}
rf_preds <- as.character(predict(rf_model, 
                                 newdata = test_df, 
                                 type = "prob")[["good"]] > rf_best_threshold) %>% 
    fct_recode(good = "TRUE", 
               bad = "FALSE")
rf_conf_mat <- confusionMatrix(rf_preds, test_df$outcome)
print_conf_matrix(rf_conf_mat)
```

```{r}
rf_roc <- roc(as.numeric(test_df$outcome), as.numeric(rf_preds))
auc(rf_roc)
```

# XGBoost with AUC Objective

Now do the same with XGBoost. There are many hyperparameters to tune, and using grid search through these would not be possible in a reasonable time. Instead I will use Bayesian Optimisation via the `rBayesianOptimization` package.

```{r}
# create matrices with Matrix::sparse.model.matrix 
train_sparse <- sparse.model.matrix(outcome ~ ., data = train_df)
test_sparse <- sparse.model.matrix(outcome ~ ., data = test_df)
# xgboost needs binary numeric labels
train_labels <- if_else(train_df$outcome == "good", 1L, 0L)
test_labels <- if_else(test_df$outcome == "good", 1L, 0L)

# xgb performs best with its own object type
dtrain <- xgb.DMatrix(train_sparse, label = train_labels)
dtest <- xgb.DMatrix(test_sparse, label = test_labels)

# set seed for reproducibility
set.seed(1907L)

# define the folds for cross-validation
cv_folds <- KFold(train_labels, nfolds = 5,
                  stratified = TRUE, seed = 0)

# create a function that will return values for the optimisation
xgb_cv_bayes <- function(max_depth, 
                         min_child_weight, 
                         subsample, 
                         eta,
                         colsample_bytree, 
                         lambda, 
                         alpha, 
                         gamma) {
    cv <- xgb.cv(params = list(booster = "gbtree", 
                               eta = eta,
                               max_depth = max_depth,
                               min_child_weight = min_child_weight,
                               subsample = subsample, 
                               colsample_bytree = colsample_bytree,
                               lambda = lambda, 
                               alpha = alpha,
                               gamma = gamma, 
                               objective = "binary:logistic",
                               eval_metric = "auc"),
                 data = dtrain, 
                 nrounds = 10000L,
                 folds = cv_folds, 
                 prediction = TRUE, 
                 showsd = TRUE,
                 early_stopping_rounds = 100L, 
                 maximize = TRUE, 
                 verbose = 0)
    list(Score = cv$evaluation_log$test_auc_mean[cv$best_iteration],
         Pred = cv$pred)
}

# run optimisation with bounds for the parameters to be tested
if (file.exists("./data/xgb_opt_res.rds")) {
    xgb_opt_res <- read_rds("./data/xgb_opt_res.rds")
} else {
    xgb_opt_res <- BayesianOptimization(xgb_cv_bayes,
                                        bounds = list(
                                            max_depth = c(1L, 15L),
                                            min_child_weight = c(1L, 20L), 
                                            subsample = c(0.2, 1.0), 
                                            eta = c(0.0001, 0.1), 
                                            colsample_bytree = c(0.2, 1), 
                                            lambda = c(0.5, 20.0), 
                                            alpha = c(0.5, 20.0), 
                                            gamma = c(0.0, 20.0)),
                                        init_grid_dt = NULL, 
                                        init_points = 10, 
                                        # number of random points used to 
                                        # initialise the process. Must be at 
                                        # least 1 or will fail
                                        n_iter = 50, # this is a lot of rounds 
                                        # and will take quite some time to run. 
                                        # Can get good results n_iter = 20.
                                        acq = "ucb", 
                                        kappa = 2.576, 
                                        eps = 0.0,
                                        verbose = TRUE)
    write_rds(xgb_opt_res, "./data/xgb_opt_res.rds")
}
xgb_opt_res$Best_Par
xgb_opt_res$History
```

Now run the cross-validation again with the best parameters found, to have the evaluation log.

```{r}
xgb_params <- xgb_opt_res[["Best_Par"]]
xgb_param_list <- list(
    eta = xgb_params[["eta"]], 
    max_depth = xgb_params[["max_depth"]], 
    min_child_weight = xgb_params[["min_child_weight"]], 
    subsample = xgb_params[["subsample"]], 
    colsample_bytree = xgb_params[["colsample_bytree"]], 
    lambda = xgb_params[["lambda"]], 
    alpha = xgb_params[["alpha"]], 
    gamma = xgb_params[["gamma"]], 
    objective = "binary:logistic", 
    eval_metric = "auc")

if (file.exists("./data/xgb_cv.rds")) {
    xgb_cv <- read_rds("./data/xgb_cv.rds")
} else {
    xgb_cv <- xgb.cv(params = xgb_param_list, 
                     data = dtrain, 
                     nrounds = 10000L,
                     folds = cv_folds, 
                     prediction = TRUE, 
                     showsd = TRUE,
                     early_stopping_rounds = 1000L, 
                     print_every_n = 500L, 
                     maximize = TRUE)
    
    write_rds(xgb_cv, "./data/xgb_cv.rds")
}
```

Now I will plot the training progress for cross-validation to detect bias or variance.

```{r}
xgb_cv[["evaluation_log"]] %>% 
    dplyr::select(iter, train_auc_mean, test_auc_mean) %>% 
    gather(key = "partition", 
           value = "auc", 
           train_auc_mean, 
           test_auc_mean) %>% 
    mutate(partition = stri_extract_first_regex(partition, "^[a-z]+")) %>% 
    ggplot(aes(x = iter, y = auc, colour = partition)) +
    # facet_wrap(~ cv_count) +
    geom_point() +
    scale_colour_viridis(discrete = TRUE, option = "A") + 
    ylim(0, 1) +
    theme_minimal()
```

Now train the XGBoost model.

```{r}
xgb_model <- xgb.train(params = xgb_param_list, 
                       data = dtrain, 
                       nrounds = xgb_cv[["best_iteration"]], 
                       metrics = "auc")
```

Can visualise the relative importance of the features using functions from `xgboost`.

```{r}
var_importance <- xgb.importance(model = xgb_model)
xgb.plot.importance(var_importance)
```

Now I need to set the prediction threshold on the test set.

```{r}
xgb_preds <- predict(xgb_model, dtest)
thresholds <- seq(0.01, 0.99, 0.01)
auc_scores <- map_dbl(thresholds, 
                      function(x) {
                          tmp_preds <- as.integer(predict(xgb_model, dtest) > x)
                          auc(roc(test_labels, tmp_preds))
                      })

xgb_best_threshold <- thresholds[[which.max(auc_scores)]]
paste0("Threshold that maximises AUC is ", xgb_best_threshold)
```

Now evaluate the model on the test set.

```{r}
xgb_factor_preds <- as.character(xgb_preds > xgb_best_threshold) %>% 
    fct_recode(good = "TRUE", 
               bad = "FALSE")
xgb_conf_mat <- confusionMatrix(xgb_factor_preds, test_df$outcome)
print_conf_matrix(xgb_conf_mat)
```

```{r}
xgb_roc <- roc(as.numeric(test_df$outcome), as.numeric(xgb_factor_preds))
auc(xgb_roc)
```

AUC of 0.7952 is a good result.

[1]: https://archive.ics.uci.edu/ml/index.php
[2]: https://xgboost.ai/